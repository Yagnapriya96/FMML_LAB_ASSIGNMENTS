{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMml0eimswzraCuM8XK7nK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yagnapriya96/FMML_LAB_ASSIGNMENTS/blob/main/stock%20\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VmjttnrdBFn"
      },
      "outputs": [],
      "source": [
        "Project Breakdown\n",
        "Data Collection\n",
        "\n",
        "Stock Data: You’ll need historical stock data (open, high, low, close prices, and volume). You can use APIs such as Yahoo Finance, Alpha Vantage, or Quandl to collect this data.\n",
        "Data Preprocessing\n",
        "\n",
        "Data Cleaning: Handle missing values and normalize or standardize data where necessary.\n",
        "Feature Engineering: Calculate additional features like moving averages, RSI (Relative Strength Index), or Bollinger Bands that might help improve predictions.\n",
        "Modeling\n",
        "\n",
        "Time Series Analysis: You can use classical models like ARIMA or modern deep learning models like LSTM (Long Short-Term Memory) networks for stock prediction.\n",
        "Regression Models: Use linear regression, decision trees, or Random Forest as a baseline.\n",
        "Evaluation\n",
        "\n",
        "Performance Metrics: Use Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared to evaluate the model performance.\n",
        "Backend & Frontend Development\n",
        "\n",
        "Backend: Use Flask/Django for creating APIs that will serve predictions.\n",
        "Frontend: Use HTML/CSS/JavaScript with React or simple frontend templates to display results.\n",
        "Database: Store historical stock data and predictions in MongoDB.\n",
        "\n",
        "Deployment: Deploy your web app using platforms like Heroku, AWS, or DigitalOcean.\n",
        "\n",
        "Step-by-Step Procedure\n",
        "1. Install Required Libraries\n",
        "You need the following Python libraries to get started:\n",
        "\n",
        "pip install pandas numpy scikit-learn keras tensorflow matplotlib seaborn flask pymongo requests yfinance\n",
        "pandas & numpy: Data manipulation.\n",
        "scikit-learn: Machine learning models.\n",
        "keras & tensorflow: Deep learning models (LSTM).\n",
        "matplotlib & seaborn: Data visualization.\n",
        "flask: Backend for serving predictions.\n",
        "pymongo: MongoDB integration.\n",
        "requests & yfinance: To fetch stock data from Yahoo Finance.\n",
        "2. Data Collection\n",
        "Use yfinance to fetch stock data:\n",
        "\n",
        "import yfinance as yf\n",
        "\n",
        "def fetch_data(stock_symbol, start_date, end_date):\n",
        "    stock_data = yf.download(stock_symbol, start=start_date, end=end_date)\n",
        "    return stock_data\n",
        "\n",
        "# Example: Fetch data for 'AAPL' (Apple) from Jan 1, 2020 to Feb 1, 2025\n",
        "data = fetch_data('AAPL', '2020-01-01', '2025-02-01')\n",
        "print(data.head())\n",
        "3. Data Preprocessing\n",
        "Handle missing values and normalize the data:\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def preprocess_data(data):\n",
        "    data = data[['Close']]  # Using only the closing prices for simplicity\n",
        "    data.fillna(method='ffill', inplace=True)  # Forward fill missing values\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "    return scaled_data, scaler\n",
        "\n",
        "scaled_data, scaler = preprocess_data(data)\n",
        "4. Modeling\n",
        "Let's use an LSTM model for stock price prediction. You could also experiment with a simple linear regression model or ARIMA for comparison.\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "# Prepare data for LSTM\n",
        "def create_dataset(data, time_step=60):\n",
        "    X, y = [], []\n",
        "    for i in range(time_step, len(data)):\n",
        "        X.append(data[i-time_step:i, 0])  # Using previous 'time_step' values as features\n",
        "        y.append(data[i, 0])  # Predicting the next value\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Reshape data for LSTM\n",
        "X, y = create_dataset(scaled_data)\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)  # LSTM expects 3D input\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=50, return_sequences=True, input_shape=(X.shape[1], 1)))\n",
        "model.add(LSTM(units=50, return_sequences=False))\n",
        "model.add(Dense(units=1))  # Predicting one value (the next price)\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=10, batch_size=32)\n",
        "5. Evaluate the Model\n",
        "Once the model is trained, you can make predictions and evaluate the performance.\n",
        "\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# Reverse scaling to get actual stock prices\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "actual = scaler.inverse_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Calculate RMSE\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "rmse = math.sqrt(mean_squared_error(actual, predictions))\n",
        "print(f'RMSE: {rmse}')\n",
        "6. Create Backend API (Flask)\n",
        "Now, create a Flask API to serve predictions.\n",
        "\n",
        "from flask import Flask, jsonify, request\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "\n",
        "app = Flask(_name_)\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('stock_prediction_model.h5')\n",
        "\n",
        "@app.route('/predict', methods=['GET'])\n",
        "def predict():\n",
        "    stock_symbol = request.args.get('symbol')\n",
        "    data = fetch_data(stock_symbol, '2020-01-01', '2025-02-01')\n",
        "    data = preprocess_data(data)\n",
        "    X, y = create_dataset(data)\n",
        "\n",
        "    prediction = model.predict(X[-1].reshape(1, 60, 1))  # Predict next stock price\n",
        "    predicted_price = scaler.inverse_transform(prediction)\n",
        "\n",
        "    return jsonify({'predicted_price': predicted_price[0][0]})\n",
        "\n",
        "if _name_ == '_main_':\n",
        "    app.run(debug=True)\n",
        "7. Frontend\n",
        "Create a simple frontend to display the predictions. Here's a simple example in HTML/JS:\n",
        "\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <title>Stock Price Prediction</title>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Stock Price Prediction</h1>\n",
        "    <label for=\"symbol\">Enter Stock Symbol:</label>\n",
        "    <input type=\"text\" id=\"symbol\" name=\"symbol\" required>\n",
        "    <button onclick=\"predict()\">Predict</button>\n",
        "    <h2>Predicted Price: <span id=\"predicted_price\"></span></h2>\n",
        "\n",
        "    <script>\n",
        "        async function predict() {\n",
        "            const symbol = document.getElementById('symbol').value;\n",
        "            const response = await fetch(/predict?symbol=${symbol});\n",
        "            const data = await response.json();\n",
        "            document.getElementById('predicted_price').innerText = data.predicted_price;\n",
        "        }\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "8. MongoDB Integration\n",
        "To store predictions and data, integrate MongoDB using pymongo:\n",
        "\n",
        "from pymongo import MongoClient\n",
        "\n",
        "client = MongoClient(\"mongodb://localhost:27017/\")\n",
        "db = client['stock_data']\n",
        "collection = db['predictions']\n",
        "\n",
        "def save_prediction(symbol, predicted_price):\n",
        "    prediction = {\n",
        "        \"symbol\": symbol,\n",
        "        \"predicted_price\": predicted_price,\n",
        "        \"timestamp\": pd.Timestamp.now()\n",
        "    }\n",
        "    collection.insert_one(prediction)\n",
        "9. Deployment\n",
        "Prepare for Deployment:\n",
        "\n",
        "Make sure you have your Flask app running properly.\n",
        "Push your code to GitHub or any other code repository.\n",
        "Deploy on Heroku:\n",
        "\n",
        "Install Heroku CLI.\n",
        "Create a Procfile for Heroku:\n",
        "web: python app.py\n",
        "Create a requirements.txt for dependencies:\n",
        "pip freeze > requirements.txt\n",
        "Push to Heroku:\n",
        "git push heroku master\n",
        "Alternatively, you can deploy on AWS EC2, DigitalOcean, or other hosting platforms.\n",
        "\n",
        "Conclusion\n",
        "You now have an outline for creating a stock price prediction project with historical data, machine learning, and deep learning. You'll gather stock data, preprocess it, train a model, create a backend API, and deploy it. If you have specific questions or need further details about any part of the process, feel free to ask!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "import pickle\n",
        "\n",
        "# Step 1: Data Collection\n",
        "def get_stock_data(ticker, start_date, end_date):\n",
        "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
        "    return stock_data\n",
        "\n",
        "# Step 2: Data Preprocessing\n",
        "def preprocess_data(stock_data):\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(stock_data['Close'].values.reshape(-1, 1))\n",
        "    return scaled_data, scaler\n",
        "\n",
        "# Step 3: Create Sequences for LSTM\n",
        "def create_sequences(data, seq_length):\n",
        "    sequences, labels = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        sequences.append(data[i:i + seq_length])\n",
        "        labels.append(data[i + seq_length])\n",
        "    return np.array(sequences), np.array(labels)\n",
        "\n",
        "# Step 4: Model Training\n",
        "def build_train_model(X_train, y_train):\n",
        "    model = Sequential([\n",
        "        LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
        "        Dropout(0.2),\n",
        "        LSTM(units=50, return_sequences=False),\n",
        "        Dropout(0.2),\n",
        "        Dense(units=1)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
        "    return model\n",
        "\n",
        "# Step 5: Save Model & Scaler\n",
        "def save_model(model, scaler, model_filename='stock_model.h5', scaler_filename='scaler.pkl'):\n",
        "    model.save(model_filename)\n",
        "    with open(scaler_filename, 'wb') as f:\n",
        "        pickle.dump(scaler, f)\n",
        "\n",
        "# Running the pipeline\n",
        "ticker = 'AAPL'\n",
        "start_date = '2020-01-01'\n",
        "end_date = '2024-01-01'\n",
        "\n",
        "stock_data = get_stock_data(ticker, start_date, end_date)\n",
        "scaled_data, scaler = preprocess_data(stock_data)\n",
        "seq_length = 60\n",
        "X, y = create_sequences(scaled_data, seq_length)\n",
        "\n",
        "# Splitting into training and testing sets\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, y_train = X[:train_size], y[:train_size]\n",
        "X_test, y_test = X[train_size:], y[train_size:]\n",
        "\n",
        "# Train Model\n",
        "model = build_train_model(X_train, y_train)\n",
        "\n",
        "# Save Model\n",
        "save_model(model, scaler)\n",
        "\n",
        "print(\"Model and Scaler saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61hRh8yIfusq",
        "outputId": "abf894fa-29b4-4488-fda5-a6519653d7bd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YF.download() has changed argument auto_adjust default to True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - loss: 0.1211\n",
            "Epoch 2/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0059\n",
            "Epoch 3/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0049\n",
            "Epoch 4/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0051\n",
            "Epoch 5/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0043\n",
            "Epoch 6/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0041\n",
            "Epoch 7/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.0045\n",
            "Epoch 8/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 0.0037\n",
            "Epoch 9/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 0.0044\n",
            "Epoch 10/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0048\n",
            "Epoch 11/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0038\n",
            "Epoch 12/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0035\n",
            "Epoch 13/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0039\n",
            "Epoch 14/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0041\n",
            "Epoch 15/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0038\n",
            "Epoch 16/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0034\n",
            "Epoch 17/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 0.0035\n",
            "Epoch 18/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - loss: 0.0036\n",
            "Epoch 19/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0030\n",
            "Epoch 20/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0034\n",
            "Epoch 21/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0037\n",
            "Epoch 22/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0036\n",
            "Epoch 23/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0035\n",
            "Epoch 24/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0033\n",
            "Epoch 25/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0029\n",
            "Epoch 26/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - loss: 0.0036\n",
            "Epoch 27/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 0.0030\n",
            "Epoch 28/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0030\n",
            "Epoch 29/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0027\n",
            "Epoch 30/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0037\n",
            "Epoch 31/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0034\n",
            "Epoch 32/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.0036\n",
            "Epoch 33/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0026\n",
            "Epoch 34/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0030\n",
            "Epoch 35/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 0.0028\n",
            "Epoch 36/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.0026\n",
            "Epoch 37/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0028\n",
            "Epoch 38/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0037\n",
            "Epoch 39/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0027\n",
            "Epoch 40/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0027\n",
            "Epoch 41/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0025\n",
            "Epoch 42/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0033\n",
            "Epoch 43/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0026\n",
            "Epoch 44/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.0027\n",
            "Epoch 45/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - loss: 0.0027\n",
            "Epoch 46/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0030\n",
            "Epoch 47/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0033\n",
            "Epoch 48/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0025\n",
            "Epoch 49/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0026\n",
            "Epoch 50/50\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and Scaler saved successfully!\n"
          ]
        }
      ]
    }
  ]
}